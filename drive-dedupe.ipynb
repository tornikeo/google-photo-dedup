{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3a3ebcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--verbose] [--delete] [--renamed]\n",
      "                             [--query QUERY]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/tornikeo/.local/share/jupyter/runtime/kernel-b8f504da-2b9e-4cd3-8127-2b7961fa3ef4.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tornikeo/.miniconda3/envs/drive-dedupe/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3406: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "__author__ = 'theirix'\n",
    "__license__ = 'MIT'\n",
    "\"\"\"\n",
    "Google Photo Dedup.\n",
    "\n",
    "Small script for removing duplicate Google Photo uploads.\n",
    "They may appear when photos of different resolutions were backuped.\n",
    "Usually these files have same name and/or EXIF create time (do not\n",
    "confuse with Drive file createdTime) and miss EXIF information.\n",
    "\n",
    "To actually remove files specify '-d' flag. To see JSONs specify '-v' flag.\n",
    "Duplicate files are moved to Drive trash.\n",
    "\n",
    "Before first launch it is needed to setup Drive credentials at developer console\n",
    "and drop credentials JSON at ~/.config/google-photo-dedup/client_id.json\n",
    "\"\"\"\n",
    "\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive',\n",
    "          'https://www.googleapis.com/auth/drive.file',\n",
    "          'https://www.googleapis.com/auth/drive.metadata',\n",
    "          'https://www.googleapis.com/auth/drive.photos.readonly']\n",
    "\n",
    "APPLICATION_NAME = 'Google Photo Dedup'\n",
    "\n",
    "\n",
    "def get_credentials():\n",
    "    \"\"\"Gets valid user credentials from storage.\n",
    "\n",
    "    If nothing has been stored, or if the stored credentials are invalid,\n",
    "    the OAuth2 flow is completed to obtain the new credentials.\n",
    "\n",
    "    Returns:\n",
    "        Credentials, the obtained credential.\n",
    "    \"\"\"\n",
    "\n",
    "    home_dir = os.path.expanduser('~')\n",
    "    credential_dir = os.path.join(home_dir, '.config', 'google-photo-dedup')\n",
    "    if not os.path.exists(credential_dir):\n",
    "        os.makedirs(credential_dir)\n",
    "    credential_path = os.path.join(credential_dir, 'token.pickle')\n",
    "    client_secret_path = os.path.join('client_secrets.json')\n",
    "    creds = None\n",
    "    if os.path.exists(credential_path):\n",
    "        with open(credential_path, 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "\n",
    "    # If there are no (valid) credentials available, let the user log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(client_secret_path, SCOPES)\n",
    "            creds = flow.run_local_server()\n",
    "        # Save the credentials for the next run\n",
    "        print('Storing credentials to ' + credential_path)\n",
    "        with open(credential_path, 'wb') as token:\n",
    "            pickle.dump(creds, token)\n",
    "\n",
    "    return creds\n",
    "\n",
    "\n",
    "def pretty_inspect(file):\n",
    "    \"\"\" :return: pretty file representation \"\"\"\n",
    "    return \"{} ({}x{}, {} MP, {} KiB), {}\".format(\n",
    "        file.get('name'),\n",
    "        file.get('imageMediaMetadata').get('width'),\n",
    "        file.get('imageMediaMetadata').get('height'),\n",
    "        image_resolution(file) // (1024*1024),\n",
    "        (int(file.get('size')) // 1024),\n",
    "        file.get('webViewLink'))\n",
    "\n",
    "def unique_image_resolution(file):\n",
    "    \"\"\" :return: hashed image resolutions for the Drive file\"\"\"\n",
    "    imm = file.get('imageMediaMetadata')\n",
    "    return hash((imm.get('width'), imm.get('height')))\n",
    "\n",
    "def image_resolution(file):\n",
    "    \"\"\" :return: image resolutions for the Drive file\"\"\"\n",
    "    imm = file.get('imageMediaMetadata')\n",
    "    return imm.get('width') * imm.get('height')\n",
    "\n",
    "def time_key(file):\n",
    "    \"\"\" :return: 'time' field or None if absent or damaged \"\"\"\n",
    "    field = file.get('imageMediaMetadata').get('time')\n",
    "    if field and len(field) > 5:\n",
    "        return field\n",
    "    return None\n",
    "\n",
    "def group_key(file):\n",
    "    \"\"\" :return: grouping key for finding duplicates \"\"\"\n",
    "    if time_key(file):\n",
    "        return time_key(file)\n",
    "    # createdTime is not reilable because often it is upload time\n",
    "    #return file.get('createdTime')\n",
    "    # instead use drive file name. it perfectly matches our needs\n",
    "    return file.get('name')\n",
    "\n",
    "def with_camera_model(file):\n",
    "    \"\"\" :return: if camera model is present \"\"\"\n",
    "    return file.get('cameraModel') != None and file.get('cameraModel') != ''\n",
    "\n",
    "def process_group(prefered, duplicates, service, flags):\n",
    "    ever_deleted = False\n",
    "\n",
    "    # Print data\n",
    "    print(\"  Prefer: {}\".format(pretty_inspect(prefered)))\n",
    "    if flags.verbose:\n",
    "        print(\"  JSON: \" + repr(prefered))\n",
    "    for file in duplicates:\n",
    "        print(\"  Delete: {}\".format(pretty_inspect(file)))\n",
    "        if flags.verbose:\n",
    "            print(\"  JSON: \" + repr(file))\n",
    "\n",
    "    # Sanity checks\n",
    "    # Skip deleting if any duplicate has camera info while the prefered item does not\n",
    "    if not with_camera_model(prefered) and any(with_camera_model(f) for f in duplicates):\n",
    "        print(\"Ignore removing duplicates where camera model is set\")\n",
    "        return False\n",
    "    # Skip deleting if any duplicate is larger than the prefered item\n",
    "    if int(prefered.get('size')) < max(int(f.get('size')) for f in duplicates):\n",
    "        print(\"Ignore removing duplicates larger than prefered\")\n",
    "        return False\n",
    "\n",
    "    # Delete duplicates\n",
    "    for file in duplicates:\n",
    "        if flags.delete:\n",
    "            service.files().update(fileId=file.get('id'), body={'trashed': True}).execute()\n",
    "            ever_deleted = True\n",
    "\n",
    "    return ever_deleted\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "Rules for finding duplicates.\n",
    "Build equivalence groups by group_key where resolution (WxH) differ\n",
    "Then leave only these photos where resolution is biggest (better if camera\n",
    "model specified).\n",
    "Sometimes different photos can be shoot during one second so they will fall\n",
    "to the same equivalence group. It is okay because they are exluded due to\n",
    "the same resolution.\n",
    "\"\"\"\n",
    "    # For debug purpose:\n",
    "    # logging.getLogger().setLevel(logging.DEBUG)\n",
    "    # httplib2.debuglevel = 4\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--verbose', '-v', action='store_true',\n",
    "                        help='explain what is going on')\n",
    "    parser.add_argument('--delete', '-d', action='store_true',\n",
    "                        help='actually delete things')\n",
    "    parser.add_argument('--renamed', '-m', action='store_true',\n",
    "                        help='enable mode with fuzzy renamed search')\n",
    "    parser.add_argument('--query', '-q',\n",
    "                        help='additional API query')\n",
    "    flags = parser.parse_args()\n",
    "\n",
    "    credentials = get_credentials()\n",
    "    service = build('drive', 'v3', credentials=credentials)\n",
    "\n",
    "    query = \"mimeType='image/jpeg' and trashed=false\"\n",
    "    # query += \" and createdTime >= '2016-01-02'\"\n",
    "    # query += \" and name contains '2016-01-02'\"\n",
    "    # query += \" and name contains '2012-05-21'\"\n",
    "    # query += \" and (name contains '20120513_' or name contains '2012_05_13')\"\n",
    "    if flags.query and len(flags.query) > 0:\n",
    "        query += \" \" + flags.query\n",
    "\n",
    "    files_list = []\n",
    "    page_token = None\n",
    "    page_index = 0\n",
    "\n",
    "    if flags.delete:\n",
    "        print(\"DELETE mode\")\n",
    "\n",
    "    print(\"Fetching metadata \", end=\"\")\n",
    "    while True:\n",
    "        response = service.files().list(\n",
    "            q=query,\n",
    "            spaces='drive',\n",
    "            fields=\"nextPageToken,\" +\n",
    "            \"files(id,name,size,modifiedTime,createdTime,ownedByMe,webViewLink,\"\n",
    "            + \"imageMediaMetadata(width,height,cameraModel,time))\",\n",
    "            orderBy='createdTime',\n",
    "            pageToken=page_token).execute()\n",
    "        files_list += response.get('files', [])\n",
    "        page_token = response.get('nextPageToken', None)\n",
    "        if page_token is None:\n",
    "            break\n",
    "        page_index += 1\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "\n",
    "    print(\"\\nFound files: {}, fetched {} pages\".format(\n",
    "        len(files_list), page_index))\n",
    "\n",
    "    print(\"Files with 'time' metadata: {}\".format(\n",
    "        len(list(f for f in files_list if time_key(f)))))\n",
    "\n",
    "    if flags.verbose:\n",
    "        for file in files_list:\n",
    "            print(repr(file), \"\\n\")\n",
    "\n",
    "    files_list = list(f for f in files_list if f.get('ownedByMe'))\n",
    "\n",
    "    # Filter photos\n",
    "    print(\"Stage 1: searching for duplicates by name groups\")\n",
    "    duplicates_groups = list([k, list(v)]\n",
    "                             for k, v in itertools.groupby(\n",
    "                                 sorted(\n",
    "                                     (f for f in files_list if f.get('ownedByMe') and group_key(f)),\n",
    "                                     key=group_key),\n",
    "                                 group_key))\n",
    "    duplicates_groups = list([key, sorted(duplicates,\n",
    "                                          key=image_resolution)]\n",
    "                             for key, duplicates in duplicates_groups\n",
    "                             if len(duplicates) > 1 and\n",
    "                             len(set(unique_image_resolution(f) for f in duplicates)) > 1)\n",
    "    print(\"Found duplicate groups: {}\".format(len(duplicates_groups)))\n",
    "\n",
    "    # Iterate duplicate groups\n",
    "    ever_deleted = False\n",
    "    for key, duplicates in duplicates_groups:\n",
    "        print(\n",
    "            \"\\nProcessing duplicates for createdTime {} - {} photo(s)\".format(\n",
    "                key, len(duplicates)))\n",
    "        prefered = duplicates.pop()\n",
    "        if process_group(prefered, duplicates, service, flags):\n",
    "            ever_deleted = True\n",
    "\n",
    "    # Iterate by small time steps\n",
    "    if not ever_deleted and flags.renamed:\n",
    "        print(\"Stage 2: searching for duplicates by fuzzy name search\")\n",
    "        for file in files_list:\n",
    "            name = file.get('name')\n",
    "            match = re.match(r\"^(\\d{4})-(\\d{2})-(\\d{2}) (\\d{2})\\.(\\d{2})\\.(\\d{2})(.*)\", name)\n",
    "            if match:\n",
    "                m = match.groups()\n",
    "                name_time = time.mktime((int(m[0]), int(m[1]), int(m[2]),\n",
    "                                         int(m[3]), int(m[4]), int(m[5]), 0, 0, 0))\n",
    "                for delta in [-1, 0, +1]:\n",
    "                    duplicate_name = time.strftime(\"IMG_%Y%m%d_%H%M%S\",\n",
    "                                                   time.localtime(name_time+delta)) + m[6]\n",
    "                    duplicates = list(dfile for dfile in files_list if\n",
    "                                      dfile.get('name') == duplicate_name and\n",
    "                                      image_resolution(dfile) < image_resolution(file))\n",
    "                    if len(duplicates) > 0:\n",
    "                        print(\"\\nProcessing duplicates for name {} and time delta={}\".format(\n",
    "                            name, delta))\n",
    "                        process_group(file, duplicates, service, flags)\n",
    "\n",
    "\n",
    "    print(\"Done\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8f2c17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import itertools\n",
    "import argparse\n",
    "import pickle\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daf095db",
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = 'theirix'\n",
    "__license__ = 'MIT'\n",
    "\"\"\"\n",
    "Google Photo Dedup.\n",
    "\n",
    "Small script for removing duplicate Google Photo uploads.\n",
    "They may appear when photos of different resolutions were backuped.\n",
    "Usually these files have same name and/or EXIF create time (do not\n",
    "confuse with Drive file createdTime) and miss EXIF information.\n",
    "\n",
    "To actually remove files specify '-d' flag. To see JSONs specify '-v' flag.\n",
    "Duplicate files are moved to Drive trash.\n",
    "\n",
    "Before first launch it is needed to setup Drive credentials at developer console\n",
    "and drop credentials JSON at ~/.config/google-photo-dedup/client_id.json\n",
    "\"\"\"\n",
    "\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive',\n",
    "          'https://www.googleapis.com/auth/drive.file',\n",
    "          'https://www.googleapis.com/auth/drive.metadata',\n",
    "          'https://www.googleapis.com/auth/drive.photos.readonly']\n",
    "\n",
    "APPLICATION_NAME = 'Google Photo Dedup'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d764e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_credentials():\n",
    "    \"\"\"Gets valid user credentials from storage.\n",
    "\n",
    "    If nothing has been stored, or if the stored credentials are invalid,\n",
    "    the OAuth2 flow is completed to obtain the new credentials.\n",
    "\n",
    "    Returns:\n",
    "        Credentials, the obtained credential.\n",
    "    \"\"\"\n",
    "\n",
    "    home_dir = os.path.expanduser('~')\n",
    "    credential_dir = os.path.join(home_dir, '.config', 'google-photo-dedup')\n",
    "    if not os.path.exists(credential_dir):\n",
    "        os.makedirs(credential_dir)\n",
    "    credential_path = os.path.join(credential_dir, 'token.pickle')\n",
    "    client_secret_path = os.path.join('client_secrets.json')\n",
    "    creds = None\n",
    "    if os.path.exists(credential_path):\n",
    "        with open(credential_path, 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "\n",
    "    # If there are no (valid) credentials available, let the user log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(client_secret_path, SCOPES)\n",
    "            creds = flow.run_local_server()\n",
    "        # Save the credentials for the next run\n",
    "        print('Storing credentials to ' + credential_path)\n",
    "        with open(credential_path, 'wb') as token:\n",
    "            pickle.dump(creds, token)\n",
    "\n",
    "    return creds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d540e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pretty_inspect(file):\n",
    "    \"\"\" :return: pretty file representation \"\"\"\n",
    "    return \"{} ({}x{}, {} MP, {} KiB), {}\".format(\n",
    "        file.get('name'),\n",
    "        file.get('imageMediaMetadata').get('width'),\n",
    "        file.get('imageMediaMetadata').get('height'),\n",
    "        image_resolution(file) // (1024*1024),\n",
    "        (int(file.get('size')) // 1024),\n",
    "        file.get('webViewLink'))\n",
    "\n",
    "def unique_image_resolution(file):\n",
    "    \"\"\" :return: hashed image resolutions for the Drive file\"\"\"\n",
    "    imm = file.get('imageMediaMetadata')\n",
    "    return hash((imm.get('width'), imm.get('height')))\n",
    "\n",
    "def image_resolution(file):\n",
    "    \"\"\" :return: image resolutions for the Drive file\"\"\"\n",
    "    imm = file.get('imageMediaMetadata')\n",
    "    return imm.get('width') * imm.get('height')\n",
    "\n",
    "def time_key(file):\n",
    "    \"\"\" :return: 'time' field or None if absent or damaged \"\"\"\n",
    "    field = file.get('imageMediaMetadata').get('time')\n",
    "    if field and len(field) > 5:\n",
    "        return field\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf71158a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def group_key(file):\n",
    "    \"\"\" :return: grouping key for finding duplicates \"\"\"\n",
    "    if time_key(file):\n",
    "        return time_key(file)\n",
    "    # createdTime is not reilable because often it is upload time\n",
    "    #return file.get('createdTime')\n",
    "    # instead use drive file name. it perfectly matches our needs\n",
    "    return file.get('name')\n",
    "\n",
    "def with_camera_model(file):\n",
    "    \"\"\" :return: if camera model is present \"\"\"\n",
    "    return file.get('cameraModel') != None and file.get('cameraModel') != ''\n",
    "\n",
    "def process_group(prefered, duplicates, service, flags):\n",
    "    ever_deleted = False\n",
    "\n",
    "    # Print data\n",
    "    print(\"  Prefer: {}\".format(pretty_inspect(prefered)))\n",
    "    if flags.verbose:\n",
    "        print(\"  JSON: \" + repr(prefered))\n",
    "    for file in duplicates:\n",
    "        print(\"  Delete: {}\".format(pretty_inspect(file)))\n",
    "        if flags.verbose:\n",
    "            print(\"  JSON: \" + repr(file))\n",
    "\n",
    "    # Sanity checks\n",
    "    # Skip deleting if any duplicate has camera info while the prefered item does not\n",
    "    if not with_camera_model(prefered) and any(with_camera_model(f) for f in duplicates):\n",
    "        print(\"Ignore removing duplicates where camera model is set\")\n",
    "        return False\n",
    "    # Skip deleting if any duplicate is larger than the prefered item\n",
    "    if int(prefered.get('size')) < max(int(f.get('size')) for f in duplicates):\n",
    "        print(\"Ignore removing duplicates larger than prefered\")\n",
    "        return False\n",
    "\n",
    "    # Delete duplicates\n",
    "    for file in duplicates:\n",
    "        if flags.delete:\n",
    "            service.files().update(fileId=file.get('id'), body={'trashed': True}).execute()\n",
    "            ever_deleted = True\n",
    "\n",
    "    return ever_deleted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33a046ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "Rules for finding duplicates.\n",
    "Build equivalence groups by group_key where resolution (WxH) differ\n",
    "Then leave only these photos where resolution is biggest (better if camera\n",
    "model specified).\n",
    "Sometimes different photos can be shoot during one second so they will fall\n",
    "to the same equivalence group. It is okay because they are exluded due to\n",
    "the same resolution.\n",
    "\"\"\"\n",
    "    # For debug purpose:\n",
    "    # logging.getLogger().setLevel(logging.DEBUG)\n",
    "    # httplib2.debuglevel = 4\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--verbose', '-v', action='store_true',\n",
    "                        help='explain what is going on')\n",
    "    parser.add_argument('--delete', '-d', action='store_true',\n",
    "                        help='actually delete things')\n",
    "    parser.add_argument('--renamed', '-m', action='store_true',\n",
    "                        help='enable mode with fuzzy renamed search')\n",
    "    parser.add_argument('--query', '-q',\n",
    "                        help='additional API query')\n",
    "    flags = parser.parse_args()\n",
    "\n",
    "    credentials = get_credentials()\n",
    "    service = build('drive', 'v3', credentials=credentials)\n",
    "\n",
    "    query = \"mimeType='image/jpeg' and trashed=false\"\n",
    "    # query += \" and createdTime >= '2016-01-02'\"\n",
    "    # query += \" and name contains '2016-01-02'\"\n",
    "    # query += \" and name contains '2012-05-21'\"\n",
    "    # query += \" and (name contains '20120513_' or name contains '2012_05_13')\"\n",
    "    if flags.query and len(flags.query) > 0:\n",
    "        query += \" \" + flags.query\n",
    "\n",
    "    files_list = []\n",
    "    page_token = None\n",
    "    page_index = 0\n",
    "\n",
    "    if flags.delete:\n",
    "        print(\"DELETE mode\")\n",
    "\n",
    "    print(\"Fetching metadata \", end=\"\")\n",
    "    while True:\n",
    "        response = service.files().list(\n",
    "            q=query,\n",
    "            spaces='drive',\n",
    "            fields=\"nextPageToken,\" +\n",
    "            \"files(id,name,size,modifiedTime,createdTime,ownedByMe,webViewLink,\"\n",
    "            + \"imageMediaMetadata(width,height,cameraModel,time))\",\n",
    "            orderBy='createdTime',\n",
    "            pageToken=page_token).execute()\n",
    "        files_list += response.get('files', [])\n",
    "        page_token = response.get('nextPageToken', None)\n",
    "        if page_token is None:\n",
    "            break\n",
    "        page_index += 1\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "\n",
    "    print(\"\\nFound files: {}, fetched {} pages\".format(\n",
    "        len(files_list), page_index))\n",
    "\n",
    "    print(\"Files with 'time' metadata: {}\".format(\n",
    "        len(list(f for f in files_list if time_key(f)))))\n",
    "\n",
    "    if flags.verbose:\n",
    "        for file in files_list:\n",
    "            print(repr(file), \"\\n\")\n",
    "\n",
    "    files_list = list(f for f in files_list if f.get('ownedByMe'))\n",
    "\n",
    "    # Filter photos\n",
    "    print(\"Stage 1: searching for duplicates by name groups\")\n",
    "    duplicates_groups = list([k, list(v)]\n",
    "                             for k, v in itertools.groupby(\n",
    "                                 sorted(\n",
    "                                     (f for f in files_list if f.get('ownedByMe') and group_key(f)),\n",
    "                                     key=group_key),\n",
    "                                 group_key))\n",
    "    duplicates_groups = list([key, sorted(duplicates,\n",
    "                                          key=image_resolution)]\n",
    "                             for key, duplicates in duplicates_groups\n",
    "                             if len(duplicates) > 1 and\n",
    "                             len(set(unique_image_resolution(f) for f in duplicates)) > 1)\n",
    "    print(\"Found duplicate groups: {}\".format(len(duplicates_groups)))\n",
    "\n",
    "    # Iterate duplicate groups\n",
    "    ever_deleted = False\n",
    "    for key, duplicates in duplicates_groups:\n",
    "        print(\n",
    "            \"\\nProcessing duplicates for createdTime {} - {} photo(s)\".format(\n",
    "                key, len(duplicates)))\n",
    "        prefered = duplicates.pop()\n",
    "        if process_group(prefered, duplicates, service, flags):\n",
    "            ever_deleted = True\n",
    "\n",
    "    # Iterate by small time steps\n",
    "    if not ever_deleted and flags.renamed:\n",
    "        print(\"Stage 2: searching for duplicates by fuzzy name search\")\n",
    "        for file in files_list:\n",
    "            name = file.get('name')\n",
    "            match = re.match(r\"^(\\d{4})-(\\d{2})-(\\d{2}) (\\d{2})\\.(\\d{2})\\.(\\d{2})(.*)\", name)\n",
    "            if match:\n",
    "                m = match.groups()\n",
    "                name_time = time.mktime((int(m[0]), int(m[1]), int(m[2]),\n",
    "                                         int(m[3]), int(m[4]), int(m[5]), 0, 0, 0))\n",
    "                for delta in [-1, 0, +1]:\n",
    "                    duplicate_name = time.strftime(\"IMG_%Y%m%d_%H%M%S\",\n",
    "                                                   time.localtime(name_time+delta)) + m[6]\n",
    "                    duplicates = list(dfile for dfile in files_list if\n",
    "                                      dfile.get('name') == duplicate_name and\n",
    "                                      image_resolution(dfile) < image_resolution(file))\n",
    "                    if len(duplicates) > 0:\n",
    "                        print(\"\\nProcessing duplicates for name {} and time delta={}\".format(\n",
    "                            name, delta))\n",
    "                        process_group(file, duplicates, service, flags)\n",
    "\n",
    "\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7603a9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--verbose] [--delete] [--renamed]\n",
      "                             [--query QUERY]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/tornikeo/.local/share/jupyter/runtime/kernel-b8f504da-2b9e-4cd3-8127-2b7961fa3ef4.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
